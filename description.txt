Есть csv файл, расположенный по ссылке. Скачивайте. Там внутри 3 файла. 

Так как работа дата инженера не всегда легка и проста, то иногда приходится разбираться с тем, какие данные находятся в файлах. Но, схему мы Вам все таки дадим :)

Файл с данными о полетах:

DATE: Дата полета.
DAY_OF_WEEK: День недели.
AIRLINE: Авиалиния.
FLIGHT_NUMBER: Номер рейса.
TAIL_NUMBER: Номер самолета.
ORIGIN_AIRPORT: Аэропорт отправления (IATA код).
DESTINATION_AIRPORT: Аэропорт назначения (IATA код).
DEPARTURE_DELAY: Задержка при вылете (в минутах).
DISTANCE: Расстояние (в милях).
ARRIVAL_DELAY: Задержка при прибытии (в минутах).
DIVERTED: Был ли рейс перенаправлен.
CANCELLED: Был ли рейс отменен.
CANCELLATION_REASON: Причина отмены (A - авиалиния, B - погода, C - NAS, D - безопасность).
AIR_SYSTEM_DELAY: Задержка по вине системы авиадиспетчерского управления.
SECURITY_DELAY: Задержка по причинам безопасности.
AIRLINE_DELAY: Задержка по вине авиалинии.
LATE_AIRCRAFT_DELAY: Задержка по вине позднего прибытия предыдущего рейса.
WEATHER_DELAY: Задержка из-за погоды.
DEPARTURE_HOUR: Час вылета.
ARRIVAL_HOUR: Час прибытия.
Файл с данными о аэропортах:

IATA CODE: Код IATA.
Airport: Название аэропорта.
City: Город.
Latitude: Широта.
Longitude: Долгота.
Файл с данными о авиалиниях:

IATA CODE: Код IATA.
AIRLINE: Название авиалинии.
А что, если мы (аналитик) все же ошиблись/ся с названиями или определениями полей? Это необходимо проверить!

Что необходимо сделать? Начнем с простого.

 

-1. Обратите внимание, что в проекте используются PySpark и Clickhouse. PySpark в airflow в docker-compose нету! Его необходимо добавить самостоятельно. Куда вставить в docker-compose - задача самостоятельная. 

0. Предварительно разверните docker-compose из первых степов! Шаги 1-7 должны быть сделаны в DAG.

1. Загрузите файл данных в DataFrame PySpark. Обязательно выведите количество строк.

2. Убедитесь, что данные корректно прочитаны (правильный формат, отсутствие пустых строк).

3. Преобразуйте текстовые и числовые поля в соответствующие типы данных (например, дата, число).

4. Найдите топ-5 авиалиний с наибольшей средней задержкой.

5. Вычислите процент отмененных рейсов для каждого аэропорта.

6. Определите, какое время суток (утро, день, вечер, ночь) чаще всего связано с задержками рейсов.

7. Добавьте в данные о полетах новые столбцы, рассчитанные на основе существующих данных:

IS_LONG_HAUL: Флаг, указывающий, является ли рейс дальнемагистральным (если расстояние больше 1000 миль).
DAY_PART: Определите, в какое время суток происходит вылет (утро, день, вечер, ночь).
С данными поработали, проанализировали, теперь можно их и в БД положить. 

8. Создайте схему таблицы в PostgreSQL, которая будет соответствовать структуре ваших данных. PostgreSQL уже находится в docker-compose! Схему нужно создать вне Airflow, например через Dbeaver.

9. Настройте соединение с PostgreSQL из кода, но из PySpark. (обязательно сделать это нужно в Airflow)

10. Загрузите только 10.000 строк из DataFrame в таблицу в PostgreSQL. (обязательно сделать это нужно в Airflow)

11. Выполните SQL скрипт в Python-PySpark скрипте, который выведет компанию - общее время полетов ее самолетов.

В качестве решения необходимо прислать ссылку на репозиторий. Обратите внимание на критерии (их 4) сдачи проекта - 

1. Оформление. Репозиторий оформлен так, как нужно. А именно есть структура, удобное содержание, читаемый README, естественно наличие задания. Обязательно наличие Docker файла.

2. Логика. Все сделано в одном скрипте. Сначала читаем данные с csv, потом преобразуем данные, анализируем, загружаем в postgresql, читаем с postgresql.

3. Содержание. Файл c csv хранить в GIT нельзя! Добавляйте либо ссылку, либо срез данных.

4. Корректность. Данные нельзя редактировать, файл нельзя обрезать. Результаты аналитики и SQL скрипта должны быть правильными.